{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peZ8uwPzkmK-"
      },
      "source": [
        "![UCA](http://univ-cotedazur.fr/fr/university/communication-presse/charte-et-logos/logo/png/uca-logo-ligne)\n",
        "# **DEEP LEARNING SCHOOL - LAB SESSION**\n",
        "\n",
        "## **Object Detection Using Deep learning**\n",
        "\n",
        "\n",
        "#### Lab proposed by:\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Frederic PRECIOSO\n",
        "Inria - Laboratoire I3S - Full Professor at Universite Cote d'Azur\n",
        "\n",
        "frederic.precioso@unice.fr /\n",
        "[linkedin](https://www.linkedin.com/in/frédéric-precioso-3a37389) /\n",
        "[web page](http://www.i3s.unice.fr/~precioso)\n",
        "\n",
        "### Melissa SANABRIA\n",
        "Inria - Laboratoire I3S - Universite Cote d'Azur / Wildmoka\n",
        "\n",
        "sanabria@i3s.unice.fr /\n",
        "[linkedin](http://linkedin.com/in/msanabriar/) /\n",
        "[web page](http://www.i3s.unice.fr/~sanabria/)\n",
        "\n",
        "\n",
        "### Alan FERBACH\n",
        "Videtics / Universite Cote d'Azur\n",
        "\n",
        "alan.ferbach@videtics.com /\n",
        "[linkedin](https://www.linkedin.com/in/alan-ferbach/)\n",
        "\n",
        "\n",
        "### Pierre-Alexis LE BORGNE\n",
        "Videtics / Universite Cote d'Azur\n",
        "\n",
        "pierre.alexis.le.borgne@videtics.com /\n",
        "[linkedin](https://www.linkedin.com/in/pierre-alexis-le-borgne-279894121/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQQ72CrJnMas"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6vaDqfonHyw"
      },
      "source": [
        "#Requirements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mmy2RFSt9lOE"
      },
      "source": [
        "All the lines of code in this lab will assume you can read and write the files from the folder *DLS-2019-Object-Detection* located in your Drive account. For this reason, is important you grant access to your Drive files from this notebook. This will also store all the modifications in your Drive account.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_Wmibv0cvQt"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DthPdZhPbk_m"
      },
      "source": [
        "The following lines should be executed only once at the beginning of the lab. They will download all the files you need to work on the lab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhkDiy-rbs9b"
      },
      "source": [
        "!wget --no-check-certificate -r 'https://docs.google.com/uc?export=download&id=1iS1hhDaftfITMHomMPRnZAwQLGROyWWy' -O folder.zip\n",
        "!mkdir /content/drive/'My Drive'/DLS-2019-Object-Detection/\n",
        "!unzip -qq folder.zip -d /content/drive/'My Drive'/DLS-2019-Object-Detection/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43LE7y7xtn3H"
      },
      "source": [
        "We strongly recommend to restart the runtime at the beginning of each Part.\n",
        "\n",
        "<img src=\"https://i.stack.imgur.com/Pr6iZ.png\" width=400/>\n",
        "\n",
        "Or  just press *Ctrl + M*. This is important to clear the gpu memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pAtE6XdBYEs"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XY9L0prJRi5g"
      },
      "source": [
        "In the lab of today we will explore different algorithms for Object Detection, Instance Segmentation and Semantic Segmentation on images.\n",
        "\n",
        "One of the most popular task of computer vision is **image Classification**, i.e. telling which object appears on a picture. However, sometimes it's not enough to know what's in an image, we might need to know where the object is in the image.\n",
        "\n",
        "**Object Detection** opens up the capability of counting how many objects are in a scene, tracking motion or simply just locating an object’s position.\n",
        "\n",
        "The output of an object detector is an array of bounding boxes around objects detected in the image or video frame, but we do not get any clue about the shape of the object inside the bounding box. **Instance Segmentation** includes identification of boundaries of the objects at the detailed pixel level.\n",
        "\n",
        "On the other hand, **Semantic Segmentation** achieves fine-grained inference by making dense predictions inferring labels for every pixel of the image, so that each pixel is labeled with the class of its enclosing object or region.\n",
        "\n",
        "<!---!<img src=\"https://cdn-images-1.medium.com/max/800/1*Y40V8ZZ9T_XI-eGQulwIRQ.png\"/>--->\n",
        "\n",
        "<img src=\"https://cdn-images-1.medium.com/max/600/1*-zw_Mh1e-8YncnokbAFWxg.png\" />\n",
        "\n",
        "**Classification**: There is a balloon in this image. **Semantic Segmentation**: These are all the balloon pixels. **Object Detection**: There are 7 balloons in this image at these locations. We’re starting to account for objects that overlap. **Instance Segmentation**: There are 7 balloons at these locations, and these are the pixels that belong to each one.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bCXSSsk5Gv5"
      },
      "source": [
        "# Part I: Object Detection\n",
        "\n",
        "In the first part of this lab we will explore Faster R-CNN  ([Ren et al., 2016](http://openaccess.thecvf.com/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf)) algorithm, since it is one of the famous object detection architectures that uses convolution neural networks and it is also a good example to show how object detection solutions have evolved.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1I0-et_dIzPw"
      },
      "source": [
        "In the folder */content/drive/My Drive/DLS-2019-Object-Detection/images* there are some images with which you can play for the object detection and instance segmentation tasks but feel free to add more images to this folder. You can modify the variable *image_name* with the image you want to try.\n",
        "\n",
        "Throughout the lab there are lines of code empty for you to complete."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9v5iuABIlGn"
      },
      "source": [
        "## R-CNN\n",
        "\n",
        "In 2014, [Girshick et al.](https://openaccess.thecvf.com/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf) proposed R-CNN (Region-based Convolutional Neural Networks).  \n",
        "\n",
        "From an input image, ~2000 bounding boxes are generated using [Selective Search](http://www.huppelen.nl/publications/selectiveSearchDraft.pdf), these bounding-boxes represent region proposals (\"Region of Interest\" or \"RoI\").\n",
        "\n",
        "Those proposed regions are cropped and warped to a fixed size. A pre-trained CNN model (like VGG or ResNet) is then used to extract a feature vector for each warped region independently.\n",
        "\n",
        "Then an SVM model is trained to classify the object in the warped image using the feature vector. A regression layer is also trained to refine the bounding box proposals\n",
        "\n",
        "<!---![](https://lilianweng.github.io/lil-log/assets/images/RCNN.png)--->\n",
        "<!---![](https://www.deeplearningitalia.com/wp-content/uploads/2018/06/2.png )--->\n",
        "<img src=\"https://www.deeplearningitalia.com/wp-content/uploads/2018/06/2.png\" width=\"500\" />\n",
        "\n",
        "However, training R-CNN is expensive and slow. Running selective search to propose 2000 regions for every image and then generate the CNN fetatures for every region in the image, takes a lot of time.\n",
        "\n",
        "Let's see how long it will take to genereate the CNN features of 2000 region proposals.\n",
        "\n",
        "In the following code we will not take into account the time of the Selective Search, for this reason, we will simulate this process generating random regions of the image. Then we will count the time a pre-trained CNN like ResNet takes to generate the features.\n",
        "\n",
        "We will use Keras API to load the pre-trained ResNet and to extract the features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHC56LZh85hk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "outputId": "39e54fd9-442a-4822-e6ab-a8518dd50c77"
      },
      "source": [
        "import time\n",
        "import cv2\n",
        "from random import randint\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.applications.resnet50 import preprocess_input\n",
        "import numpy as np\n",
        "\n",
        "image_name = \"\" # To Complete - You can choose images from /content/drive/My Drive/DLS-2019-Object-Detection/images/\n",
        "image_location = \"/content/drive/My Drive/DLS-2019-Object-Detection/images/\"+image_name\n",
        "im = cv2.imread(image_location) ## Reading image\n",
        "im_h = im.shape[0]\n",
        "im_w = im.shape[1]\n",
        "n_proposals = 2000\n",
        "\n",
        "region_props = []\n",
        "input_size = 224 ## ResNet only accepts inputs of 224x224, to make it easier we will only generate region proposal of this size\n",
        "\n",
        "## The region proposals are small regions of the image. We will define this region as a set of 4 points [x_0, y_0, x_1, y_1].\n",
        "## For instance, if the image size is 500x500, a region proposal might be [0, 0, 224, 224] which is the top left corner of the image.\n",
        "\n",
        "## Let's create random region proposals in order to simulate the Selective Search process\n",
        "## Hint: You can use the function randint(init, end) to generate randomly an integer between init and end\n",
        "for i in range(n_proposals):\n",
        "\n",
        "  x_0 = randint(0, im_w - input_size) ## To Complete\n",
        "  y_0 = ## To Complete\n",
        "  x_1 = x_0 + input_size ## To Complete\n",
        "  y_1 = ## To Complete\n",
        "  region_props.append([x_0, y_0, x_1, y_1])\n",
        "\n",
        "\n",
        "## Now let's generate the CNN features for each of this region proposals\n",
        "CNN_model = ResNet50(weights='imagenet') #Load ResNet model pre-trained with ImageNet dataset\n",
        "t = time.time()\n",
        "batch_size = 100 # The CNN model processes batch_size number of images at the same time\n",
        "X = np.zeros((batch_size, input_size, input_size, 3))\n",
        "b = 0\n",
        "for prop in region_props:\n",
        "\n",
        "  img_to_pred = im[prop[1]:prop[3],prop[0]:prop[2]]\n",
        "  img_to_pred = np.expand_dims(img_to_pred, axis=0) # Add the batch dimension. From (input_size, input_size, 3) to (1, input_size, input_size, 3)\n",
        "  img_to_pred = preprocess_input(img_to_pred) # convert the images from RGB to BGR, then will zero-center each color channel with respect to the ImageNet dataset\n",
        "  if b == batch_size:\n",
        "    CNN_model.predict(X) # Extract features\n",
        "    b = 0\n",
        "    X = np.zeros((batch_size, input_size, input_size, 3))\n",
        "\n",
        "  X[b] = img_to_pred\n",
        "  b += 1\n",
        "\n",
        "print('Time to generate features for one image: {:.3f}s'.format(time.time() - t))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-de5c0a6de939>\"\u001b[0;36m, line \u001b[0;32m26\u001b[0m\n\u001b[0;31m    y_0 = ## To Complete\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mownN8f28xQ-"
      },
      "source": [
        "## Fast R-CNN\n",
        "\n",
        "To improve R-CNN,  [Girshick et al., 2015](http://openaccess.thecvf.com/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf) proposed Fast R-CNN. Instead of extracting CNN features independently for each region proposal, this model only needs one CNN forward pass per image since the features of the regions are extracted from the feature map of the entire image.\n",
        "\n",
        "<img src=\"https://www.deeplearningitalia.com/wp-content/uploads/2018/06/3-1.png\" width=\"500\" />\n",
        "<!---![](https://www.deeplearningitalia.com/wp-content/uploads/2018/06/3-1.png)--->\n",
        "\n",
        "The following steps summarize the method:\n",
        "\n",
        "1. Propose regions (Regions of Interest or \"RoIs\") by Selective Search\n",
        "2. Take a pre-trained CNN like VGG or ResNet and replace the last Max Pooling layer by a RoI Pooling layer:\n",
        "  \n",
        "  The inputs of the RoI Pooling layer are the different region proposals projected in the feature map of the entire image and the output is a fixed-length feature vector per proposal. In other words, if we compare R-CNN with Fast R-CNN: R-CNN extracts the region proposals from the original images, warpes them in a fixed size and passes each of these small images to the pre-trained CNN. On the other hand, Fast-RCNN passes only one time the entire image to the pre-trained CNN to obtain a feature map, then it extracts the region proposals from this feature map and use RoI pooling to get fixed size regions.\n",
        "3. Finally, from the fixed-length feature vector per proposal, the model branches into two output layers:\n",
        "  *   A softmax estimator where the output is a discrete probability distribution per RoI.\n",
        "  *   A bounding-box regression model which predicts offsets relative to the original RoI.\n",
        "\n",
        "Fast R-CNN performs much better than R-CNN in terms of speed. There was just one big bottleneck remaining: the selective search algorithm for generating region proposals.\n",
        "\n",
        "###RoI Pooling\n",
        "It is a type of Max Pooling which goal is to convert features of any size (h x w), into a small fixed window, (H x W). The input region is divided into H x W grids, approximately every subwindow of size h/H x w/W. Then apply max-pooling in each grid.\n",
        "\n",
        "<img src=\"https://cdn-images-1.medium.com/max/800/1*aB4gy6i8Zc3BasYaQGDVtg.png\" width=\"800\" />\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Tt2jaxuEc9L"
      },
      "source": [
        "Now let's see how long it takes the Selective Search algorithm in one image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tl1XS23xFYLD"
      },
      "source": [
        "%cd /content/drive/'My Drive'/DLS-2019-Object-Detection/selectivesearch\n",
        "!pip install selectivesearch # Selective Search algorithm taken from https://github.com/AlpacaDB/selectivesearch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAoJvz3aILP3"
      },
      "source": [
        "import selectivesearch\n",
        "import time\n",
        "import cv2\n",
        "\n",
        "image_name = \"\" # To Complete - You can choose images from /content/drive/My Drive/DLS-2019-Object-Detection/images/\n",
        "image_location = \"/content/drive/My Drive/DLS-2019-Object-Detection/images/\"+image_name\n",
        "\n",
        "im = cv2.imread(image_location) # Read the image\n",
        "t = time.time()\n",
        "img_lbl, regions = selectivesearch.selective_search(im)\n",
        "print('Time for Selective search on one single image: {:.3f}s'.format(time.time() - t))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbKjih95EYC3"
      },
      "source": [
        "## Faster R-CNN##\n",
        "An intuitive speedup solution is to integrate the region proposal algorithm into the CNN model. Faster R-CNN  ([Ren et al., 2016](http://openaccess.thecvf.com/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf)) constructs a single, unified model composed of RPN (Region Proposal Network) and Fast R-CNN with shared convolutional feature layers.\n",
        "\n",
        "Instead of using Selective Search, Ren et al. proposed to let the network learn the region proposals using RPN.\n",
        "\n",
        "<img src=\"https://lilianweng.github.io/lil-log/assets/images/faster-RCNN.png\" width=\"700\" />\n",
        "\n",
        "\n",
        "The following steps describe how the RPN model works:\n",
        "\n",
        "1. Take a pre-trained CNN like VGG or ResNet. At the last layer of the CNN, a 3×3 sliding window moves across the feature map.\n",
        "2. At the center of each sliding window, it predicts multiple regions of various scales and ratios simultaneously.\n",
        "3. For each region proposal the model predicts: a score for that region, and 4 coordinates representing the bounding box of the region.\n",
        "\n",
        "The 2k scores represent the softmax probability of each of the k bounding boxes being an “object”. Although the RPN outputs bounding box coordinates, it does not try to classify any potential object. Its real job is still proposing object regions. If an anchor box has a score above a certain threshold, that box passes forward as a region proposal.\n",
        "\n",
        "Once we have the region proposals, we get the fixed length feature vector using a RoI pooling layer. These feature vectors are then used to classify the proposals and predict the offset values for the bounding boxes. In other workds, Faster R-CNN = RPN + Fast R-CNN.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5D0v-aviNAM"
      },
      "source": [
        "Tosee the results of RPN and Faster R-CNN, we will use [Detectron](https://github.com/facebookresearch/Detectron), which  is Facebook AI Research's software system, written in Python and powered by the Caffe2 deep learning framework.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFHFD89H6AJJ"
      },
      "source": [
        "%cd /content/drive/'My Drive'/DLS-2019-Object-Detection/cocoapi/PythonAPI\n",
        "!make install\n",
        "!python setup.py install --user\n",
        "%cd /content/drive/'My Drive'/DLS-2019-Object-Detection/detectron\n",
        "!make"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DcL5Vj50SME"
      },
      "source": [
        "In the following code, you can choose the image you want to predict. As mentioned before, in the folder */content/drive/My Drive/DLS-2019-Object-Detection/images* there are some images for you to test but we encourage you to add more. You can modify the variable *image_name* with the image you want to try.\n",
        "\n",
        "\n",
        "First, we will take a look to the output of **RPN**. The goal of RPN is to predict region proposals, which are regions where  there is possibly an object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loii6j58n2K2"
      },
      "source": [
        "image_name = \"8.jpg\"\n",
        "image_location = \"/content/drive/'My Drive'/DLS-2019-Object-Detection/images/\"+image_name\n",
        "%cd /content/drive/'My Drive'/DLS-2019-Object-Detection/detectron\n",
        "!python tools/infer_rpn.py \\\n",
        "    --im $image_location \\\n",
        "    --rpn-cfg configs/12_2017_baselines/rpn_R-50-C4_1x.yaml \\\n",
        "    --rpn-pkl models/rpn.pkl\n",
        "\n",
        "from IPython.display import Image\n",
        "Image(\"/tmp/\"+image_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iynVdZa2uggl"
      },
      "source": [
        "Now, we will see the results of Faster R-CNN.\n",
        "Faster R-CNN, takes the region proposals predicted by the RPN and classifies them in the right class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3bmKnrJFSnN"
      },
      "source": [
        "image_name = \"8.jpg\"\n",
        "image_location = \"/content/drive/'My Drive'/DLS-2019-Object-Detection/images/\"+image_name\n",
        "%cd /content/drive/'My Drive'/DLS-2019-Object-Detection/detectron\n",
        "\n",
        "!python tools/infer_simple.py \\\n",
        "    --cfg configs/12_2017_baselines/e2e_faster_rcnn_R-101-FPN_2x.yaml \\\n",
        "    --image-ext jpg \\\n",
        "    --wts models/faster_rcnn.pkl \\\n",
        "$image_location\n",
        "\n",
        "from IPython.display import Image\n",
        "Image(\"/tmp/\"+image_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKUJl7QfrO9r"
      },
      "source": [
        "##YOLO##\n",
        "\n",
        "Although R-CNNs tend to be very accurate, the biggest problem with the R-CNN family of networks is their speed, they were able to predict at only 5 FPS. This is mainly due to the fact that they have two stages, one for the proposals and the second for the classification.\n",
        "\n",
        "To help increase the speed of deep learning-based object detectors, both Single Shot Detectors (SSDs) and YOLO use a one-stage detector strategy. In general, single-stage detectors tend to be less accurate than two-stage detectors but are significantly faster.\n",
        "\n",
        "In this lab, we will take a look to [YOLO](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf) (You Only Look Once: Unified, Real-Time Object Detection). This algorithm can be used for real-time detection, since it predicts at 45 FPS.\n",
        "\n",
        "YOLO first divides the input image into grids (let's say 3x3 to make a simpler illustration). And then for each grid, it predicts the bounding boxes and the corresponding class probabilities for objects.\n",
        "\n",
        "<img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/12/Screenshot-from-2018-11-15-17-46-32.png\" />\n",
        "\n",
        "The following illustration shows the ground truth representation of each grid. Assuming we divide the image into a grid of size 3x3 and there are 3 classes: Pederastian, Car and Motorcycle, the label *y* of each grid will be an eight dimensional vector.\n",
        "\n",
        "<img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/12/Screenshot-from-2018-11-15-18-01-24.png\" />\n",
        "\n",
        "*   pc defines whether an object is present in the grid or not. This value is 1 if the grid contains the midpoint of the object.\n",
        "*   bx, by, bh, bw specify the bounding box if there is an object. Where bx and by are the x and y coordinates of the midpoint of the object with respect to the current grid.\n",
        "*   c1, c2, c3 represent the classes. So, if the object is a car, c2 will be 1 and c1 & c3 will be 0.\n",
        "\n",
        "Even if an object spans out to more than one grid, it will only be assigned to a single grid in which its mid-point is located.\n",
        "\n",
        "**Anchor Boxes**\n",
        "As we said before, each grid can only identify one object. But what if there are multiple objects in a single grid? Consider as an example the following image, divided into a 3 X 3 grid:\n",
        "\n",
        "<img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/12/Screenshot-from-2018-11-17-13-20-41.png\"/>\n",
        "\n",
        "YOLO assigns an object to a grid if the midpoint of the object is located in this grid. But in the above example, the midpoint of both objects lies in the same grid. To solve this issue, the algorithm pre-defines two different shapes called anchor boxes or anchor box shapes. Now, for each grid, instead of having one output, it will have two outputs. We can always increase the number of anchor boxes as well, I have taken two here to make the concept easy to understand:\n",
        "\n",
        "<img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/12/Screenshot-from-2018-11-17-13-36-28.png\"/>\n",
        "\n",
        "The, the *y* label with 2 anchor boxes looks like:\n",
        "\n",
        " <img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/12/Screenshot-from-2018-11-17-13-33-31.png\"/>\n",
        "\n",
        "**Intersection over Union**\n",
        "\n",
        "How can we evaluate the predicted bounding box? Intersection over Union (IoU) computes the intersection over the union between the actual bounding box and the predicted one. Consider the actual (red) and the predicted (blue) bounding boxes for a car as shown below.\n",
        "\n",
        "<img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/12/Screenshot-from-2018-11-16-13-07-50.png\" height=300/> <img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/12/Screenshot-from-2018-11-16-13-12-02.png\" height=300/>\n",
        "\n",
        "IoU = Area of the yellow box (Intersection) / Area of the green box (Union)\n",
        "\n",
        "Then the prediction is good enough if the IoU is greater than a threshold.\n",
        "\n",
        "Let's create a function to find the IoU of two boxes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObWFv0aYJpEy"
      },
      "source": [
        "def iou(box1, box2):\n",
        "  ## box = [x1, y1, x2, y2] where x1, y1 is the top left corner of the box and x2, y2 is the bottom right corner of the box\n",
        "\n",
        "  # determine the (x, y)-coordinates of the intersection rectangle\n",
        "  xi1 = max(box1[0], box2[0])\n",
        "  yi1 = max(box1[1], box2[1])\n",
        "  xi2 = min(box1[2], box2[2])\n",
        "  yi2 = min(box1[3], box2[3])\n",
        "\n",
        "  # compute the area of intersection rectangle\n",
        "  inter_area = max(0, yi2 - yi1) * max(0, xi2 - xi1)\n",
        "\n",
        "  box1_area = (box1[3] - box1[1]) * (box1[2] - box1[0])\n",
        "  box2_area = (box2[3] - box2[1]) * (box2[2] - box2[0])\n",
        "\n",
        "  # compute the intersection over union by taking the intersection area and dividing it by the box1 area + box2 area - the interesection area\n",
        "  union_area = box1_area + box2_area - inter_area\n",
        "\n",
        "  iou = inter_area / float(union_area)\n",
        "\n",
        "  return iou"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xgH8dAm6GGT"
      },
      "source": [
        "Now, we can see if our function is correct. First, we can try with two identical boxes, the IoU should be 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjKo3NdN6TVC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "20e693aa-f0a0-4ce5-95b3-ce51bafed294"
      },
      "source": [
        "box1 =[0, 1, 2, 3] # To Complete\n",
        "box2 = [0, 1, 2, 3]# To Complete\n",
        "print(iou(box1, box2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2e8sVGF6lXF"
      },
      "source": [
        "In the following code, we can find the IoU of two boxes with no intersection. The IoU should be 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNBI363m60CD"
      },
      "source": [
        "box1 = # To Complete\n",
        "box2 = # To Complete\n",
        "print(iou(box1, box2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLHCh9bA6_Fh"
      },
      "source": [
        "And now, if we try with two boxes with some intersection, the IoU should be between 0 and 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9D5NamyF7Ftt"
      },
      "source": [
        "box1 = # To Complete\n",
        "box2 = # To Complete\n",
        "print(iou(box1, box2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD3fhzTeLPjY"
      },
      "source": [
        "**Non-Maximum Suppression**\n",
        "\n",
        "One of the most common problems with object detection algorithms is that rather than detecting an object just once, they might detect it multiple times. Take as an example the image below:\n",
        "\n",
        "<img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/12/Screenshot-from-2018-11-16-13-32-40.png\" />\n",
        "\n",
        "In the previous image, there are multiple boxes detecting the same car. The Non-Max Suppression technique tries to clean up this prediction to get only a single detection per object.\n",
        "\n",
        "It first takes the detection with highest probability. In the above image, it will select the bounding box with 0.9 prediction. Then it will supress the boxes with high IoU with respect to the previoulsy selected one. So, the boxes with 0.6 and 0.7 probabilities will be suppressed in our example.\n",
        "\n",
        "In the following code, we will create a nms function. But, it is important to modify our previous IoU function. The IoU function created before, finds the IoU between two boxes, but we would like to find the IoU between one box and several others."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXtsJ-ms-MqL"
      },
      "source": [
        "import numpy as np\n",
        "def iou(box, boxes):\n",
        "  ## box = [x1, y1, x2, y2] where x1, y1 is the top left corner of the box and x2, y2 is the bottom right corner of the box\n",
        "  ## boxes = N x [xn1, yn1, xn2, yn2]\n",
        "\n",
        "  # determine the (x, y)-coordinates of the intersection rectangles\n",
        "  xi1 = np.maximum(box[0], boxes[:, 0])\n",
        "  yi1 = # To Complete\n",
        "  xi2 = np.minimum(box[2], boxes[:, 2])\n",
        "  yi2 = # To Complete\n",
        "\n",
        "  # compute the area of intersection rectangles\n",
        "  inter_areas = np.maximum(0, yi2 - yi1) * np.maximum(0, xi2 - xi1)\n",
        "\n",
        "  box_area = (box[3] - box[1]) * ()# To Complete\n",
        "  boxes_area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "\n",
        "  # compute the intersection over union by taking the intersection area and dividing it by box area + boxes area - interesection areas\n",
        "  union_areas = # To Complete\n",
        "\n",
        "  iou = inter_areas / union_areas\n",
        "\n",
        "  return iou"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v0n0P-_XPKe"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Malisiewicz et al.\n",
        "def nms(boxes, scores, overlapThresh):\n",
        "\n",
        "  # if there are no boxes, return an empty list\n",
        "  if len(boxes) == 0:\n",
        "    return []\n",
        "\n",
        "  # initialize the list of picked indexes\n",
        "  pick = []\n",
        "\n",
        "  # Sort the indices by the score\n",
        "  idxs = np.argsort(scores)\n",
        "\n",
        "  # keep looping while some indexes still remain in the indexes list\n",
        "  while len(idxs) > 0:\n",
        "\n",
        "    # Choose the index with highest score and add the index value to the list of picked indexes\n",
        "    last = len(idxs) - 1\n",
        "    i = idxs[last]\n",
        "    pick.append(i)\n",
        "\n",
        "    # Get the IoU between the box with highest score and the boxes remained in the indexes list\n",
        "    overlap = iou(boxes[i], boxes[idxs[:last]])\n",
        "\n",
        "    # delete all indexes from the index list that have overlap > overlapThresh\n",
        "    idxs = np.delete(idxs, np.concatenate(([last], np.where(overlap > overlapThresh)[0])))\n",
        "\n",
        "  # return only the indices of the bounding boxes that were picked\n",
        "  return np.asarray(pick)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wjty56OCPndZ"
      },
      "source": [
        "Now, we can see the results of YOLO. It takes an input image of shape (608, 608, 3), passes this image to a convolutional neural network (CNN), which returns a (19, 19, 425) dimensional output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdVT6bwQIfDZ"
      },
      "source": [
        "%cd /content/drive/'My Drive'/DLS-2019-Object-Detection/yolo\n",
        "\n",
        "from keras.models import load_model\n",
        "from yolo_utils import preprocess_image, read_classes, read_anchors\n",
        "import time\n",
        "\n",
        "# Load pre-trained YOLO model in COCO dataset\n",
        "yolo_model = load_model(\"model_data/yolo.h5\")\n",
        "\n",
        "# You can also choose an image from /content/drive/My Drive/DLS-2019-Object-Detection/images\n",
        "img_path = \"test2.jpg\"\n",
        "\n",
        "image, image_data = preprocess_image(img_path, model_image_size = (608, 608))\n",
        "width_image, height_image = image.size\n",
        "\n",
        "start_time = time.time()\n",
        "yolo_outputs = yolo_model.predict(image_data)\n",
        "print(\"YOLO Predcition time per image:\", time.time() - start_time)\n",
        "print(yolo_outputs.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2sO0NBSjP3L"
      },
      "source": [
        "Here, each cell of a 19 X 19 grid returns 425 numbers\n",
        "\n",
        "425 = 5 * 85, where 5 is the number of anchor boxes per grid\n",
        "\n",
        "85 = 5 + 80, where 5 is (pc, bx, by, bh, bw) and 80 is the number of classes we want to detect\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/enggen/Deep-Learning-Coursera/1407e19c98833d2686a0748db26b594f3102301e/Convolutional%20Neural%20Networks/Week3/Car%20detection%20for%20Autonomous%20Driving/nb_images/flatten.png\" />\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gAoxcvbkiB5"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from yolo_utils import yolo_boxes_to_corners, scale_boxes, yolo_head, generate_colors, draw_boxes\n",
        "\n",
        "# Get the class names and the anchors\n",
        "class_names = read_classes(\"model_data/coco_classes.txt\")\n",
        "anchors = read_anchors(\"model_data/yolo_anchors.txt\")\n",
        "\n",
        "print(\"Classes:\", class_names)\n",
        "print(\"Number of classes:\", len(class_names))\n",
        "print(\"Number of anchors:\", len(anchors))\n",
        "\n",
        "## Convert final layer features to bounding box parameters\n",
        "# box_confidence : bc\n",
        "# box_xy : (bx, by)\n",
        "# box_wh : (bh, bw)\n",
        "# box_class_probs : class probability\n",
        "box_confidence, box_xy, box_wh, box_class_probs = yolo_head(yolo_outputs, anchors, len(class_names))\n",
        "\n",
        "# Convert YOLO box predictions to bounding box corners\n",
        "boxes = yolo_boxes_to_corners(box_xy, box_wh)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svyPyCVQZovI"
      },
      "source": [
        "Now, for each box (of each cell) we will compute the following elementwise product and extract a probability that the box contains a certain class.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/enggen/Deep-Learning-Coursera/1407e19c98833d2686a0748db26b594f3102301e/Convolutional%20Neural%20Networks/Week3/Car%20detection%20for%20Autonomous%20Driving/nb_images/probability_extraction.png\" />\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1Ed37tdT5bb"
      },
      "source": [
        "# As bc defines whether an object is present in the grid or not, we multiply the box confidence by the class probabilty\n",
        "box_scores = box_confidence *  box_class_probs\n",
        "\n",
        "# We obtain the class of each box by choosing the index with highest score among the 80 classes\n",
        "box_classes = np.argmax(box_scores, -1)\n",
        "\n",
        "# We obtain the class score of each box by choosing the highest score among the 80 classes\n",
        "box_class_scores = np.max(box_scores,-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX4y6ScbVjFi"
      },
      "source": [
        "Let's filter the predictions, assuming the possitive predictions are the ones with a class score higher than a threshold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4XBTwYzVWo-"
      },
      "source": [
        "threshold = 0.6\n",
        "ids_to_filter = box_class_scores > threshold\n",
        "\n",
        "scores = box_class_scores[ids_to_filter] # Select only the scores higher than 0.6\n",
        "boxes = boxes[ids_to_filter] # Select only the boxes with score higher than 0.6\n",
        "classes = box_classes[ids_to_filter] # Select the classes of the boxes with socre higher than 0.6\n",
        "\n",
        "# Scale the predicted boxes to the size of the original image\n",
        "boxes = scale_boxes(boxes, (height_image, width_image))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0N-SRFjbSUC"
      },
      "source": [
        "Before using nms, we can see that even after filtering with the threshold, there are some overlapped boxes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9y4a4MmfuBB"
      },
      "source": [
        "from PIL import Image\n",
        "#Generate colors for drawing bounding boxes.\n",
        "colors = generate_colors(class_names)\n",
        "\n",
        "# Draw bounding boxes on the image file\n",
        "image = Image.open(img_path)\n",
        "draw_boxes(image, scores, boxes, classes, class_names, colors)\n",
        "\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jO_-uMjkbTQm"
      },
      "source": [
        "print(\"Number of boxes before NMS:\", len(boxes))\n",
        "\n",
        "# Apply NMS to filter overlapping boxes\n",
        "nms_th = 0.5\n",
        "ids_nms = nms(boxes, scores, nms_th) # To complete\n",
        "print(\"Number of boxes after NMS:\", len(ids_nms))\n",
        "\n",
        "# Choose only the socres, boxes and classes chosen by nms - Hint: Same process as for the threshold\n",
        "scores = scores[ids_nms]# To complete\n",
        "boxes = boxes[ids_nms]# To complete\n",
        "classes = classes[ids_nms]# To complete\n",
        "\n",
        "# Draw bounding boxes on the image file\n",
        "image = Image.open(img_path)\n",
        "draw_boxes(image, scores, boxes, classes, class_names, colors)\n",
        "\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10pgkz5DWyxg"
      },
      "source": [
        "Here you have the entire code to detect objects with YOLO, you can play with the parameters on different images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exTOaQjlX6YB"
      },
      "source": [
        "threshold = # To Complete\n",
        "nms_th = # To Complete\n",
        "img_path = \"\" # To Complete - You can choose images from /content/drive/My Drive/DLS-2019-Object-Detection/images/\n",
        "\n",
        "%cd /content/drive/'My Drive'/DLS-2019-Object-Detection/yolo\n",
        "\n",
        "from keras.models import load_model\n",
        "from yolo_utils import preprocess_image, read_classes, read_anchors, yolo_boxes_to_corners, scale_boxes, yolo_head, generate_colors, draw_boxes\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load pre-trained YOLO model in COCO dataset\n",
        "yolo_model = load_model(\"model_data/yolo.h5\")\n",
        "\n",
        "\n",
        "image, image_data = preprocess_image(img_path, model_image_size = (608, 608))\n",
        "width_image, height_image = image.size\n",
        "\n",
        "yolo_outputs = yolo_model.predict(image_data)\n",
        "print(yolo_outputs.shape)\n",
        "\n",
        "# Get the class names and the anchors\n",
        "class_names = read_classes(\"model_data/coco_classes.txt\")\n",
        "anchors = read_anchors(\"model_data/yolo_anchors.txt\")\n",
        "colors = generate_colors(class_names)\n",
        "\n",
        "print(\"Classes:\", class_names)\n",
        "print(\"Number of classes:\", len(class_names))\n",
        "print(\"Number of anchors:\", len(anchors))\n",
        "\n",
        "## Convert final layer features to bounding box parameters\n",
        "# box_confidence : bc\n",
        "# box_xy : (bx, by)\n",
        "# box_wh : (bh, bw)\n",
        "# box_class_probs : class probability\n",
        "box_confidence, box_xy, box_wh, box_class_probs = yolo_head(yolo_outputs, anchors, len(class_names))\n",
        "\n",
        "# Convert YOLO box predictions to bounding box corners\n",
        "boxes = yolo_boxes_to_corners(box_xy, box_wh)\n",
        "\n",
        "# As bc defines whether an object is present in the grid or not, we multiply the box confidence by the class probabilty\n",
        "box_scores =  box_confidence * box_class_probs\n",
        "\n",
        "# We obtain the class of each box by choosing the index with highest score among the 80 classes\n",
        "box_classes = np.argmax(box_scores, -1)\n",
        "\n",
        "# We obtain the class score of each box by choosing the highest score among the 80 classes\n",
        "box_class_scores = np.max(box_scores,-1)\n",
        "\n",
        "ids_to_filter = box_class_scores > threshold\n",
        "\n",
        "scores = box_class_scores[ids_to_filter] # Select only the scores higher than 0.6\n",
        "boxes = boxes[ids_to_filter] # Select only the boxes with score higher than 0.6\n",
        "classes = box_classes[ids_to_filter] # Select the classes of the boxes with socre higher than 0.6\n",
        "\n",
        "# Scale the predicted boxes to the size of the original image\n",
        "boxes = scale_boxes(boxes, (height_image, width_image))\n",
        "\n",
        "print(\"Number of boxes before NMS:\", len(boxes))\n",
        "\n",
        "# Apply NMS to filter overlapping boxes\n",
        "ids_nms = nms(boxes, scores, nms_th)\n",
        "print(\"Number of boxes after NMS:\", len(ids_nms))\n",
        "\n",
        "# Choose only the socres, boxes and classes chosen by nms - Hint: Same process as for the threshold\n",
        "scores =  scores[ids_nms]\n",
        "boxes =  boxes[ids_nms]\n",
        "classes =  classes[ids_nms]\n",
        "\n",
        "# Draw bounding boxes on the image file\n",
        "image = Image.open(img_path)\n",
        "draw_boxes(image, scores, boxes, classes, class_names, colors)\n",
        "\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.imshow(image)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2aChU70amZ7"
      },
      "source": [
        "### Try YOLO on Videos\n",
        "\n",
        "As mentioned before, YOLO is fast enough to generate predictions in real time. For this reason, we will use YOLO to track elements in a video file.\n",
        "\n",
        "First, let's create some functions to group the prediction per image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42KiTXUcN0S9"
      },
      "source": [
        "import cv2\n",
        "def preprocess_image_cv2(img, model_image_size):\n",
        "    resized_image = cv2.resize(img, model_image_size)\n",
        "    resized_image = resized_image / 255.\n",
        "    resized_image = np.expand_dims(resized_image, 0)  # Add batch dimension.\n",
        "    return img, resized_image\n",
        "\n",
        "def post_process_yolo(yolo_outputs):\n",
        "\n",
        "    box_confidence, box_xy, box_wh, box_class_probs = yolo_head(yolo_outputs, anchors, len(class_names))\n",
        "\n",
        "    # Convert YOLO box predictions to bounding box corners\n",
        "    boxes = yolo_boxes_to_corners(box_xy, box_wh)\n",
        "\n",
        "    # As bc defines whether an object is present in the grid or not, we multiply the box confidence by the class probabilty\n",
        "    box_scores =  box_confidence * box_class_probs\n",
        "\n",
        "    # We obtain the class of each box by choosing the index with highest score among the 80 classes\n",
        "    box_classes = np.argmax(box_scores, -1)\n",
        "\n",
        "    # We obtain the class score of each box by choosing the highest score among the 80 classes\n",
        "    box_class_scores = np.max(box_scores,-1)\n",
        "\n",
        "    ids_to_filter = box_class_scores > threshold\n",
        "\n",
        "    scores = box_class_scores[ids_to_filter] # Select only the scores higher than 0.6\n",
        "    boxes = boxes[ids_to_filter] # Select only the boxes with score higher than 0.6\n",
        "    classes = box_classes[ids_to_filter] # Select the classes of the boxes with socre higher than 0.6\n",
        "\n",
        "    # Scale the predicted boxes to the size of the original image\n",
        "    boxes = scale_boxes(boxes, (height_image, width_image))\n",
        "\n",
        "\n",
        "    # Apply NMS to filter overlapping boxes\n",
        "    ids_nms = nms(boxes, scores, nms_th)\n",
        "\n",
        "    # Choose only the socres, boxes and classes chosen by nms - Hint: Same process as for the threshold\n",
        "    scores =  scores[ids_nms]\n",
        "    boxes =  boxes[ids_nms]\n",
        "    classes =  classes[ids_nms]\n",
        "\n",
        "    # Draw bounding boxes on the image file\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    image = Image.fromarray(img_rgb)\n",
        "    draw_boxes(image, scores, boxes, classes, class_names, colors, verbose=False)\n",
        "\n",
        "    return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cAjg3urgmGk"
      },
      "source": [
        "Due to implementation issues in Colab, we can't see the prediction in real time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVyWXllHdGTT"
      },
      "source": [
        "import cv2\n",
        "\n",
        "input_video = \"/content/drive/My Drive/DLS-2019-Object-Detection/yolo/video_1.mp4\"\n",
        "\n",
        "nms_th = 0.5\n",
        "threshold = 0.6\n",
        "\n",
        "flux = cv2.VideoCapture(input_video) # Capture input_video\n",
        "\n",
        "fps = flux.get(cv2.CAP_PROP_FPS)\n",
        "frame_count = int(flux.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "duration = frame_count/fps\n",
        "print(\"Duration of the video:\", duration, \"seconds\")\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')# FourCC is a 4-byte code used to specify the video codec. The list of available codes can be found in fourcc.org\n",
        "\n",
        "frame_width = int(flux.get(3))\n",
        "frame_height = int(flux.get(4))\n",
        "\n",
        "out = cv2.VideoWriter('output.avi',fourcc, flux.get(cv2.CAP_PROP_FPS), (frame_width,frame_height)) # We should specify the output file name, the FourCC code, number of frames per second (fps) and frame size\n",
        "\n",
        "\n",
        "while True:\n",
        "\n",
        "  ret_val, img = flux.read()\n",
        "\n",
        "  if not ret_val:\n",
        "    break\n",
        "\n",
        "  image, image_data = preprocess_image_cv2(img, model_image_size = (608, 608))\n",
        "  height_image, width_image, channel = image.shape\n",
        "\n",
        "  yolo_outputs = yolo_model.predict(image_data)\n",
        "\n",
        "  image = post_process_yolo(yolo_outputs)\n",
        "\n",
        "  img = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "  out.write(img)\n",
        "\n",
        "print(\"Prediction finished. Results stores in output.avi\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekqcKifgYyeq"
      },
      "source": [
        "The predictions are stored into the video file *output.avi*. The following code will download the video to your Drive folder */content/drive/My Drive/DLS-2019-Object-Detection/yolo/* If you open that file, you can take a look of how it would be the prediction in real-time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv7HYo3yYyKN"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('output.avi')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CR71sGanz5cd"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "#Part II: Instance Segmentation#\n",
        "\n",
        "In this second part of the lab, we will work on Instance Segmentation. We will see Mask R-CNN, you will see the results of a pre-trained network where you can compare the results of Instance Segmentation and Object Detection. Then you will finetune the network on your own dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-51mXK6M7jIk"
      },
      "source": [
        "## Mask R-CNN\n",
        "\n",
        "Mask R-CNN ([He et al., 2016](http://openaccess.thecvf.com/content_ICCV_2017/papers/He_Mask_R-CNN_ICCV_2017_paper.pdf)) uses Faster R-CNN to not only detect objects but also to perform pixel-level image segmentation.\n",
        "\n",
        "The key point is to decouple the classification and the pixel-level mask prediction tasks. Based on the framework of Faster R-CNN, Mask R-CNN added a third branch for predicting an object mask in parallel with the existing branches for classification and localization. The mask branch is a small network applied to each RoI, predicting a segmentation mask in a pixel-to-pixel manner.\n",
        "\n",
        "<img src=\"https://lilianweng.github.io/lil-log/assets/images/mask-rcnn.png\" width=\"600\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RH9jnGpDCD-f"
      },
      "source": [
        "image_name = \"7.jpg\"\n",
        "image_location = \"/content/drive/'My Drive'/DLS-2019-Object-Detection/images/\"+image_name\n",
        "%cd /content/drive/'My Drive'/DLS-2019-Object-Detection/detectron\n",
        "!python tools/infer_simple.py \\\n",
        "    --cfg configs/12_2017_baselines/e2e_mask_rcnn_R-101-FPN_2x.yaml \\\n",
        "    --image-ext jpg \\\n",
        "    --wts models/mask_rcnn.pkl \\\n",
        "$image_location\n",
        "\n",
        "from IPython.display import Image\n",
        "Image(\"/tmp/\"+image_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvzCFKq2n5uL"
      },
      "source": [
        "Now let's train Mask R-CNN with our own dataset.\n",
        "\n",
        "To save some time, we will use the dog dataset provided by [RomRoc](https://github.com/RomRoc/maskrcnn_train_tensorflow_colab/blob/master/images.zip) but you can create your own dataset using [VIA (VGG Image Annotator)](http://www.robots.ox.ac.uk/~vgg/software/via/). VIa a single HTML file that you download and open in a browser. Annotating the first few images might be slow, but once you get used to the user interface, it gets faster.\n",
        "\n",
        "If you create your own dataset, your data folder should have the following structure:\n",
        "\n",
        "\n",
        "```\n",
        "|- \"train\" directory\n",
        "    |- jpg image files of training data\n",
        "  |- \"via_region_data.json\" annotations file of training data\n",
        "|- \"val\" directory\n",
        "  |- jpg image files of validation data\n",
        "  |- \"via_region_data.json\" annotations file of validation data\n",
        "```\n",
        "\n",
        "*via_region_data.json* file is generated by VIA tool.\n",
        "\n",
        "Let's take a look to the images of the dog dataset. You can modify the variable *nb_imgs_to_show* to check more images or you can re-run the following code, each time it choses randomly the images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vF1LNM166xLi"
      },
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "from random import shuffle\n",
        "\n",
        "dataset_path = \"/content/drive/My Drive/DLS-2019-Object-Detection/Mask_RCNN/dataset/train/\"\n",
        "images_list = glob.glob(dataset_path + \"*.jpg\")\n",
        "shuffle(images_list)\n",
        "\n",
        "nb_imgs_to_show = # To Complete - Choose the number of images you want to check. From 1 to 50\n",
        "\n",
        "fig=plt.figure(figsize=(20,50))\n",
        "\n",
        "for i in range(nb_imgs_to_show):\n",
        "\n",
        "  fig.add_subplot(1, nb_imgs_to_show, i+1)\n",
        "  plt.imshow(Image.open(images_list[i]))\n",
        "  plt.axis('off')\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFUroFfHHRg6"
      },
      "source": [
        "With the following code you can see the bounding boxes and the pixel labeling of some images on the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkVapSR3Fj5f"
      },
      "source": [
        "%cd /content/drive/'My Drive'/DLS-2019-Object-Detection/Mask_RCNN/\n",
        "\n",
        "import dog\n",
        "from mrcnn import utils\n",
        "from mrcnn import visualize\n",
        "\n",
        "# Load training dataset\n",
        "dataset = dog.DogDataset()\n",
        "dataset.load_dog(\"/content/drive/My Drive/DLS-2019-Object-Detection/Mask_RCNN/dataset/\", \"train\")\n",
        "dataset.prepare()\n",
        "\n",
        "nb_imgs_to_show = # To Complete - Choose the number of images you want to check. From 1 to 50\n",
        "image_ids = dataset.image_ids\n",
        "shuffle(image_ids)\n",
        "\n",
        "for image_id in image_ids[:nb_imgs_to_show]:\n",
        "  image = dataset.load_image(image_id)\n",
        "  mask, class_ids = dataset.load_mask(image_id)\n",
        "  bbox = utils.extract_bboxes(mask)\n",
        "  visualize.display_instances(image, bbox, mask, class_ids, dataset.class_names, figsize=(4, 4))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0IU_evQXPrK"
      },
      "source": [
        "The Mask R-CNN network that we will use to detect the dog in our images has the following configuration: the pre-trained CNN architecture to extract features is [ResNet101](http://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf), the RPN and the mask branch were trained using [COCO](http://cocodataset.org/) dataset.\n",
        "\n",
        "The next command, will finetune the Mask R-CNN network using the images located in the *train* folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-J_ORIM06dv"
      },
      "source": [
        "%cd /content/drive/'My Drive'/DLS-2019-Object-Detection/Mask_RCNN\n",
        "!python dog.py train --dataset=dataset/ --epochs=5 --weights=coco # You can modify the number of epochs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nMPjHxTA0fG"
      },
      "source": [
        "After the training with our own dataset, we will see the results. With the variabla *epoch_to_test* you can choose with which epoch of the training you want to test, like that you can check the quality of the predictions after each epoch of training.\n",
        "\n",
        "First, let's see how is the prediction after training only for 1 epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AAyRVUBUEcC"
      },
      "source": [
        "%cd /content/drive/'My Drive'/DLS-2019-Object-Detection/Mask_RCNN\n",
        "\n",
        "epoch_to_test = 1 ## You can choose the epoch\n",
        "\n",
        "from mrcnn.visualize import display_images\n",
        "import mrcnn.model as modellib\n",
        "import dog\n",
        "import glob\n",
        "import tensorflow as tf\n",
        "from mrcnn import visualize\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "custom_DIR = \"/content/drive/My Drive/DLS-2019-Object-Detection/Mask_RCNN/dataset\"\n",
        "MODEL_DIR = \"/content/drive/My Drive/DLS-2019-Object-Detection/Mask_RCNN/logs\"\n",
        "custom_WEIGHTS_PATH = sorted(glob.glob(\"logs/*/mask_rcnn_*.h5\"))[epoch_to_test -1]\n",
        "\n",
        "config = dog.DogConfig()\n",
        "class InferenceConfig(config.__class__):\n",
        "    # Run detection on one image at a time\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "\n",
        "config = InferenceConfig()\n",
        "\n",
        "\n",
        "# Load validation dataset\n",
        "dataset = dog.DogDataset()\n",
        "dataset.load_dog(custom_DIR, \"val\")\n",
        "dataset.prepare()\n",
        "\n",
        "with tf.device(\"/gpu:0\"):\n",
        "    model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR,\n",
        "                              config=config)\n",
        "\n",
        "print(\"Loading weights \", custom_WEIGHTS_PATH)\n",
        "model.load_weights(custom_WEIGHTS_PATH, by_name=True)\n",
        "\n",
        "for image_id in dataset.image_ids:\n",
        "  image = dataset.load_image(image_id)\n",
        "\n",
        "  # Run object detection\n",
        "  results = model.detect([image])\n",
        "\n",
        "  # Display results\n",
        "  r = results[0]\n",
        "  visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'],\n",
        "                              dataset.class_names, r['scores'],\n",
        "                              title=\"Predictions\", figsize=(5, 10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Za4kLaXhRpex"
      },
      "source": [
        "Now, let's see how is the prediction after training for 5 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TNLXsEcRb3B"
      },
      "source": [
        "%cd /content/drive/'My Drive'/DLS-2019-Object-Detection/Mask_RCNN\n",
        "\n",
        "epoch_to_test = 5 ## You can choose the epoch\n",
        "\n",
        "from mrcnn.visualize import display_images\n",
        "import mrcnn.model as modellib\n",
        "import dog\n",
        "import glob\n",
        "import tensorflow as tf\n",
        "from mrcnn import visualize\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "custom_DIR = \"/content/drive/My Drive/DLS-2019-Object-Detection/Mask_RCNN/dataset\"\n",
        "MODEL_DIR = \"/content/drive/My Drive/DLS-2019-Object-Detection/Mask_RCNN/logs\"\n",
        "custom_WEIGHTS_PATH = sorted(glob.glob(\"logs/*/mask_rcnn_*.h5\"))[epoch_to_test -1]\n",
        "\n",
        "config = dog.DogConfig()\n",
        "class InferenceConfig(config.__class__):\n",
        "    # Run detection on one image at a time\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "\n",
        "config = InferenceConfig()\n",
        "\n",
        "# Load validation dataset\n",
        "dataset = dog.DogDataset()\n",
        "dataset.load_dog(custom_DIR, \"val\")\n",
        "dataset.prepare()\n",
        "\n",
        "with tf.device(\"/gpu:0\"):\n",
        "    model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR,\n",
        "                              config=config)\n",
        "\n",
        "print(\"Loading weights \", custom_WEIGHTS_PATH)\n",
        "model.load_weights(custom_WEIGHTS_PATH, by_name=True)\n",
        "\n",
        "for image_id in dataset.image_ids:\n",
        "  image = dataset.load_image(image_id)\n",
        "\n",
        "  # Run object detection\n",
        "  results = model.detect([image])\n",
        "\n",
        "  # Display results\n",
        "  r = results[0]\n",
        "  visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'],\n",
        "                              dataset.class_names, r['scores'],\n",
        "                              title=\"Predictions\", figsize=(5, 10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0Iao9PEVXlc"
      },
      "source": [
        "#Part III: Semantic Segmentation#\n",
        "\n",
        "Finally, we will work on Segmentic Segmentation.\n",
        "\n",
        "Usually CNN architectures are composed by pooling layers to be robust to local image transformations. However, these pooling layers or the convolution striding, reduces the resolution of the image, which is a big problem for segmantic segmentation.\n",
        "\n",
        "To solve this issue, some approaches like [DeconvNet](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Noh_Learning_Deconvolution_Network_ICCV_2015_paper.pdf), [Segnet](https://arxiv.org/pdf/1511.00561.pdf) and [DUC](https://arxiv.org/pdf/1702.08502.pdf) use a Encoder-Decoder approach. In this type of methods the image is followed by several convolution and pooling layers which learn a small representation of the image, and then other group of layers try to recontruct the image with the predicted segmentation, usually applying deconvolution.\n",
        "\n",
        "<img src=\"http://cvlab.postech.ac.kr/research/deconvnet/images/overall.png\" width=\"900\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nacrl7Iv71Kf"
      },
      "source": [
        "## Atrous Convolution\n",
        "\n",
        "Other approaches like [DilatedNet](https://arxiv.org/pdf/1511.07122.pdf]) or [DeepLab](https://arxiv.org/pdf/1606.00915.pdf) proposed by Google, use ***Atrous Convolution*** (which is also called *Dilated Convolution*).\n",
        "\n",
        "\n",
        "<img src=\"https://cdn-images-1.medium.com/max/800/0*oX5IPr7TlVM2NpEU.gif\" width=\"300\"/>\n",
        "<img src=\"https://cdn-images-1.medium.com/max/800/0*3cTXIemm0k3Sbask.gif\" width=\"300\"/>\n",
        "\n",
        "The term “Atrous” indeed comes from French “à trous” meaning hole. Thus, it is also called “algorithme à trous” and “hole algorithm”. In regular convolution, the kernel is applied to consecutive values of the input image but in Atrous convolution, there is a stride to choose the values of the input image. The animation on the right shows an example of atrous convolution with stride r=2\n",
        "\n",
        "These approaches build models based on well-known image classification arquitectures like VGG, and replace some of the normal convolutions with atrous convolutions. This allows us to enlarge the field of view of filters to incorporate larger context and trys to overcome the reduction of resolution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_G2BQt3LvDXu"
      },
      "source": [
        "In the folder */content/drive/My Drive/DLS-2019-Object-Detection/images_segmentation* there are some images with which you can play for the semantic segmentation task but feel free to add more images to this folder. You can modify the variable *image_name* with the image you want to try.\n",
        "\n",
        "In the following exervises, we will see the results of DilatedNet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "De6qAsdns75I"
      },
      "source": [
        "%cd /content/drive/'My Drive'/DLS-2019-Object-Detection/DilatedNet\n",
        "from predict import predict\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "image_name = \"cat.jpg\"\n",
        "image_location = \"/content/drive/My Drive/DLS-2019-Object-Detection/images_segmentation/\"+image_name\n",
        "print(image_location)\n",
        "prediction = predict(image_location)\n",
        "\n",
        "fig=plt.figure(figsize=(10,30))\n",
        "\n",
        "fig.add_subplot(1, 2, 1)\n",
        "plt.imshow(Image.open(image_location))\n",
        "plt.axis('off')\n",
        "fig.add_subplot(1, 2, 2)\n",
        "plt.imshow(Image.fromarray(prediction))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3KM2jVRzrBD"
      },
      "source": [
        "In these types of models, it's interesting to analyze the output of the different layers. The following image shows the description of the model, like that you can know the name of the layer you want to see in the next lines of code.\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1vanCGPWV5pa6WzBJW5kujVRjCpFXnCaP\" width=\"500\" />\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHRvUrvJR4Qf"
      },
      "source": [
        "First, let's see how is the output of the last convolution layer before starting the dilation *conv4_3*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmkuMnicSAZ4"
      },
      "source": [
        "%cd /content/drive/'My Drive'/DLS-2019-Object-Detection/DilatedNet\n",
        "import predict\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "image_name = \"cat.jpg\"\n",
        "image_location = \"/content/drive/My Drive/DLS-2019-Object-Detection/images_segmentation/\"+image_name\n",
        "\n",
        "layer_name = \"conv5\"\n",
        "\n",
        "out = predict.get_output_per_layer(layer_name, image_location)\n",
        "print(\"Layer \", layer_name)\n",
        "print(\"Number of feature maps: \", out.shape[-1])\n",
        "print(\"Size of each feature map: {} x {}\".format(out.shape[0], out.shape[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sScJaeWB8t6G"
      },
      "source": [
        "At the end of the execution of the previous code, you can see the number of feature maps and the size, resulting from the layer you chose. In the next lines of code, you can decide which feature maps you can to display, using the variables *init_map* and *end_map*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3k6_pLy4y67"
      },
      "source": [
        "init_map = 10\n",
        "end_map = 15\n",
        "feats_map = range(init_map, end_map)\n",
        "nb_imgs_to_show = len(feats_map)\n",
        "\n",
        "fig=plt.figure(figsize=(50,100))\n",
        "\n",
        "for i in range(nb_imgs_to_show):\n",
        "\n",
        "  fig.add_subplot(1, nb_imgs_to_show, i+1)\n",
        "  plt.imshow(Image.fromarray(out[:,:,feats_map[i]]))\n",
        "  plt.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMb5KsvPSMxb"
      },
      "source": [
        "You can modify the variable *layer_name* for any layer of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADl3K_6cSUJu"
      },
      "source": [
        "%cd /content/drive/'My Drive'/DLS-2019-Object-Detection/DilatedNet\n",
        "import predict\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "image_name = \"cat.jpg\"\n",
        "image_location = \"/content/drive/My Drive/DLS-2019-Object-Detection/images_segmentation/\"+image_name\n",
        "\n",
        "layer_name = \"\" # To Complete - Choose the layer you want to analyze\n",
        "\n",
        "out = predict.get_output_per_layer(layer_name, image_location)\n",
        "print(\"Layer \", layer_name)\n",
        "print(\"Number of feature maps: \", out.shape[-1])\n",
        "print(\"Size of each feature map: {} x {}\".format(out.shape[0], out.shape[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjcHzWY9SYcH"
      },
      "source": [
        "init_map =  # To Complete - From which feature map you want to see\n",
        "end_map = # To Complete -  Until which feature map you want to see\n",
        "feats_map = range(init_map, end_map)\n",
        "nb_imgs_to_show = len(feats_map)\n",
        "\n",
        "fig=plt.figure(figsize=(50,100))\n",
        "\n",
        "for i in range(nb_imgs_to_show):\n",
        "\n",
        "  fig.add_subplot(1, nb_imgs_to_show, i+1)\n",
        "  plt.imshow(Image.fromarray(out[:,:,feats_map[i]]))\n",
        "  plt.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xud5HvicRnw1"
      },
      "source": [
        "on"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}